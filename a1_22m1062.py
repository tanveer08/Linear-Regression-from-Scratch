# -*- coding: utf-8 -*-
"""A1_22M1062.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GqBz8Vs4ZSiYTFol94ZoVq-8Eu5CMN6A

## **Name : Tanveer Sharma** </br>
## **Roll Number : 22M1062**

# Assignment 1 Linear Regression from Scratch
Link for video explanation  : https://iitbacin-my.sharepoint.com/:v:/g/personal/22m1062_iitb_ac_in/EUw_5BZbrpNGgsB8-L063_cBFVXADajH5BHtWcqfVYBLeg?e=DsLkr2
"""

# Importing Libraries
import numpy as np
import matplotlib.pyplot as plt
import time
from sklearn.model_selection import train_test_split #only used to split dta into test and train

"""#### Task 1 Function to generate a data matrix X."""

def input_x(samples=5,dimension=3):# default sample size 5 and dimensions =3
  in_x1=np.ndarray(shape=(samples,dimension))
  for i in range(samples):
    for j in range(dimension):
       in_x1[i,j]=np.random.normal(0,5)#chosen random distribution with zero mean and variance 5
    in_x1[i,0]=1#to set first column as 1
  return in_x1

"""Comment : Here I have Chosen a normal distribution with mean 0 and variance 10 to generate the  input values"""

#testCode
np.random.seed(1)
inp_X=input_x()
print(inp_X)

"""#### Task 2 Function to generate dependent variable column t i.e. Target Value"""

def out_t(in_x,weight,w_0=2,noise_var=1):#default values 2 and 1 for w0 and noise_var respectively
  weight[0]=0# making first element zero to remove bias(may not be required)
  t=w_0*np.ones(in_x.shape[0])+in_x.dot(weight)+np.random.normal(0,noise_var,size=in_x.shape[0])#shape=0 gives samples
  return t

#testCode
np.random.seed(1)
in_x=input_x(10,5)
print(in_x.shape[0])
w=np.array([1,1,1,1,1])
w0=10
t1=out_t(in_x,w)
print(t1)

"""#### Task 3"""

def lin_reg(in_x,w):
  out_y=in_x.dot(w)#linear reg
  return out_y

#testCode
np.random.seed(1)
x1=input_x(5,3)
w1=np.array([2,2,1])
out1=lin_reg(x1,w1)
print(out1)

"""Comment : dot is used for matrix multiplication.(*) will not work here.Dot can be also changed with @ sign.

### Task 4 MSE between t and y
"""

def mean_square_error (t,y):
  mse=np.mean((t-y)**2)#np.mean is used to find mean
  return mse
#testCode
t=np.array([[1,2,3,4,5],[1,2,3,4,5]])
y=np.array([[1,3,2,5,4],[1,3,2,5,4]])
print(t,y)
mse=mean_square_error(t,y)
print(mse)

"""Comment : t.shape[1] implies columns(dimensions) ot t

### Task 5 Estimating weight using pseudo-inverse, assuming L2 regularization
"""

def weight_pseudo_in (in_x,t,parameter_lambda):
  w = (np.linalg.inv(in_x.T.dot(in_x) + parameter_lambda*np.identity(in_x.shape[1]))).dot(in_x.T.dot(t))#shape 1  is to chose dimensions size
  y=lin_reg(in_x,w)
  mse_pseudo_in=mean_square_error(t,y)
  return w,y,mse_pseudo_in
#testCode
X = input_x(12,5)
w = np.array([1,23,54,8,9])
t=out_t(X,w,5,7)
print(t)
w1,y,mse_pseudo_in=weight_pseudo_in(X,t,0.1)
print('Updated Weights',w1)
print(mse_pseudo_in)
print(y)

"""Comment : x.shape[1] implies columns(dimensions) ot x.It has been used $ϕ^t*ϕ$ is a square matrix of form [dimension,dimension].

#### Task 6 Gradient of MSE wrt W
"""

def gradient_mse(X,t,w):
    y = lin_reg(X,w)
    error = y-t
    grad = 2*X.T.dot(error)
    return grad

#testCode
X = input_x(5,2)
t = np.array([12, 12, 13,15,12])
w = np.array([1, 2])
print(X.dot(w))
grad = gradient_mse(X, t, w)
print(grad)

"""#### Task 7 L2 Norm of Weight Vector W(excluding w0)"""

def l2_norm (w):
  return np.linalg.norm(w[1:])#excluding first element
#testCode
w=np.array([1,3,5,7])
out=l2_norm(w)
print(out)

"""#### Task 8 Gradient of L2 norm with respect to the weight vectors"""

def grad_l2_wrt_weight(weight):
  gradient_l2=2*weight
  gradient_l2[0]=0#to discard first value here
  return gradient_l2
#testCode
weight=np.array([2,-4,5])
grad_l2=grad_l2_wrt_weight(weight)
print(grad_l2)

"""#### Task 9 L1 Norm of W (excluding w0)"""

def l1_norm (w):
  return np.linalg.norm(w[1:],ord=1)#order 1 is used here in np.norm for l1 norm
#testCode
w=np.array([5,3,6,2])
out=l1_norm(w)
print(out)

"""#### Task 10 Gradient of L1 norm with respect to the weight vectors"""

def grad_l1_wrt_weight(weight):
  gradient_l1=np.sign(weight)
  gradient_l1[0]=0#to discard first value here
  return gradient_l1
#testCode
weight=np.array([2,-4,5])
grad_l1=grad_l1_wrt_weight(weight)
print(grad_l1)

"""#### Task 11 Single update of Weights of Linear Regression using Gradient Descent"""

def lin_reg_single_update(X, t, w, eta, lambda2=0, lambda1=0):
    y = lin_reg(X,w)# Predict y
    error = y - t #Calculating_error
    gradient = 2 * X.T.dot(error)/2*X.shape[0] + lambda2*grad_l2_wrt_weight(w) + lambda1*grad_l1_wrt_weight(w)  #Calculating_gradient
    print(gradient)
    w = w - eta * gradient # Updatating  weight
    mse=mean_square_error(t,y) # Calculating MSE
    return w, mse

# testCode
np.random.seed(1)
X = input_x(12,5)
w = np.array([1,23,54,8,9])
t=out_t(X,w)
eta = 0.1
lambda2 = 1
lambda1 = 1
weight_uptdated,mse_updated = lin_reg_single_update(X, t, w, eta, lambda2, lambda1)
print("weight after First update:", weight_uptdated)
print("Mean Square Error",mse_updated)



"""#### Task 12 Function to estimate the weights of linear regression using gradient descent."""

def linear_regression_grad_desc(X, t, w, eta, lambda2, lambda1, min_change_NRMSE, max_iter):
    # Initialize variables
    number_iterations = 0
    old_nrmse = .0550 #Taking any value so that absolute(old_nrmse-new_nrmse) is not zero
    new_nrmse = 0.078
    while( number_iterations < max_iter and  min_change_NRMSE<abs(old_nrmse - new_nrmse)) :
        y = lin_reg(X,w)  # Predicting y
        error = y - t # Calculating error
        gradient = 2 * X.T.dot(error)/(2*X.shape[0]) + lambda2*grad_l2_wrt_weight(w) + lambda1*grad_l1_wrt_weight(w)  # Calculating gradient
        w = w - eta * gradient # Updating weights
        mse=mean_square_error(t,y)  # Calculating NRMSE
        old_nrmse = new_nrmse 
        new_nrmse = np.sqrt((mse))/np.var(t)
        #print(new_nrmse)
        number_iterations = number_iterations+1
    return w,new_nrmse,number_iterations
#check whether divide by 2N
#testCode
np.random.seed(1)
X = input_x(12,5)
w = np.array([1,23,54,8,9])
t=out_t(X,w)
eta = 0.01
lambda2 = 1
lambda1 = 1
min_change_NRMSE = 0.00000001
max_iter = 1000
x_train, x_val, t_train, t_test = train_test_split(X,t, test_size=0.2)
final_w, final_nrmse ,no_iter= linear_regression_grad_desc(x_train, t_train, w, eta, lambda2, lambda1, min_change_NRMSE, max_iter)
t_predict=lin_reg(x_val,final_w)
print("Actual t",t_test)
print("predicted t",t_predict)
print("Final weight:", final_w)
print("Final NRMSE:", final_nrmse)
print("Total Number of Iterations:",no_iter)

x=np.array([[1,2,34.4,6],[2,4,6,8]])
print(x.shape[0])

"""#### Task 13

Part a) Training and validation NRMSE obtained using pseudo inverse with number of training samples
- In this I have done 3  different codes which are explained below. 
- First code represents Change in NRMSE by changing the no of training samples (by changing test size)and keeping the total number of samples constant.
- Second code represents Change in NRMSE by changing the no of total samples and keeping the test size constant.
- Third code represents Change in NRMSE by changing seed in the train and test data with test size and number of samples constant.
"""

#let the no of sample(fixed=1000)s and d denote dimensions(fixed=5)
n=10#1number of different seed
w_initialise=np.array([0.5,20,45,6,6])#any random value
#fixing i for multiple  seed value data 
# seed values are taken from 13 to 23 
nrmse_train_seed=np.zeros(n)
nrmse_test_seed=np.zeros(n)
X_a = input_x(1000,5)
t_a=out_t(X_a,w_initialise)
for i in range(n):
  x_train, x_val, t_train, t_test = train_test_split(X_a, t_a, test_size=0.2, random_state=13+i)
  w_train,y_train,mse_pseudo_in_train=weight_pseudo_in(x_train,t_train,0.1)
  t_predict=lin_reg(x_val,w_train)
  nrmse_test=np.sqrt((mean_square_error(t_test,t_predict)))/np.var(t_test)
  nrmse_train=np.sqrt(mse_pseudo_in_train)/np.var(t_train)
  nrmse_train_seed[i]=nrmse_train
  nrmse_test_seed[i]=nrmse_test
# print("Weights in train data :",w_train)
# print("Weights in test data :",w_test)
print("Train NRMSE with increasing seed : ",nrmse_train_seed )
print("Test NRMSE with increasing seed: ",nrmse_test_seed )
print("Train NRMSE Variance : ",np.var(nrmse_train_seed))
print("Train NRMSE Mean : ",np.mean(nrmse_train_seed))
print("Test NRMSE Variance : ",np.var(nrmse_test_seed))
print(" Test NRMSE Mean : ",np.mean(nrmse_test_seed))

# create the figure and axes
fig, ax = plt.subplots()
# create the boxplots
bp1 = ax.boxplot(nrmse_train_seed, positions=[1], widths=0.6)
bp2 = ax.boxplot(nrmse_test_seed, positions=[2], widths=0.6)
# customize the boxplots
for box in bp1['boxes']:
    box.set(color='blue', linewidth=2)
for box in bp2['boxes']:
    box.set(color='green', linewidth=2)
# set the x-axis labels
ax.set_xticklabels(['Train Data', 'Test Data'])
plt.show()
plt.title('Change in nrmse due to seed')
plt.xlabel("Seed")
plt.ylabel("NRMSE")
plt.plot(np.arange(n)+13,nrmse_train_seed,label ='Train Data')
plt.plot(np.arange(n)+13,nrmse_test_seed,label ='Test Data')
plt.legend(loc = "upper right")
plt.figure(figsize=(4, 2))
plt.show()

"""As we are taking different seed we can see there is more fluctuation in test data (which is natural) and less fluctuation in train data"""

#let n denote the no of samples and d denote dimensions(fixed=5)
train_size=np.array([0.4,0.5,0.6,0.7,0.8,0.9])
w_initialise=np.array([0.5,20,45,6,6])#any random value
#fixing i for multiple  seed value data 
# seed values are taken from 13 to 23 
nrmse_train_seed=np.zeros(np.size(train_size))
nrmse_test_seed=np.zeros(np.size(train_size))
X_a = input_x(1000,5)
np.random.seed()
t_a=out_t(X_a,w_initialise,5,10)
for i in range(np.size(train_size)):
  x_train, x_val, t_train, t_test = train_test_split(X_a, t_a, test_size=train_size[i])
  w_train,y_train,mse_pseudo_in_train=weight_pseudo_in(x_train,t_train,0.1)
  t_predict=lin_reg(x_val,w_train)
  nrmse_test=np.sqrt((mean_square_error(t_test,t_predict)))/np.var(t_test)
  nrmse_train=np.sqrt(mse_pseudo_in_train)/np.var(t_train)
  nrmse_train_seed[i]=nrmse_train
  nrmse_test_seed[i]=nrmse_test
# print("Weights in train data :",w_train)
# print("Weights in test data :",w_test)
print("Train NRMSE with increasing training samples size: ",nrmse_train_seed )
print("Test NRMSE with increasing training samples size: ",nrmse_test_seed )
print("Train NRMSE Variance : ",np.var(nrmse_train_seed))
print("Train NRMSE Mean : ",np.mean(nrmse_train_seed))
print("Test NRMSE Variance : ",np.var(nrmse_test_seed))
print(" Test NRMSE Mean : ",np.mean(nrmse_test_seed))

# create the figure and axes
fig, ax = plt.subplots()
# create the boxplots
bp1 = ax.boxplot(nrmse_train_seed, positions=[1], widths=0.6)
bp2 = ax.boxplot(nrmse_test_seed, positions=[2], widths=0.6)
# customize the boxplots
for box in bp1['boxes']:
    box.set(color='blue', linewidth=2)
for box in bp2['boxes']:
    box.set(color='green', linewidth=2)
# set the x-axis labels
ax.set_xticklabels(['Train Data', 'Test Data'])
plt.show()
plt.title('Change in nrmse due to Test Size')
plt.xlabel("Test Size")
plt.ylabel("NRMSE")
plt.plot(train_size,nrmse_train_seed,label ='Train Data')
plt.plot(train_size,nrmse_test_seed,label ='Test Data')
plt.legend(loc = "upper right")
plt.figure(figsize=(4, 2))
plt.show()

"""As number of sample are constant variance changes non linearly with different seeds."""

#let n denote the no of samples and d denote dimensions(fixed=5)
samples=np.array([50,100,200,500])#number of samples
nrmse_train_seed=np.zeros(np.size(samples))
nrmse_test_seed=np.zeros(np.size(samples))
w_validate_a=np.array([0.5,20,45,6,6])
nrmse=np.zeros(samples)
for i in range(np.size(samples)):
  np.random.seed()
  X_a = input_x(samples[i],5)
  t_a=out_t(X_a,w_validate_a)
  x_train, x_val, t_train, t_test = train_test_split(X_a, t_a, test_size=0.2)
  w_train,y_train,mse_pseudo_in_train=weight_pseudo_in(x_train,t_train,0.1)
  t_predict=lin_reg(x_val,w_train)
  nrmse_test=np.sqrt((mean_square_error(t_test,t_predict)))/np.var(t_test)
  nrmse_train=np.sqrt(mse_pseudo_in_train)/np.var(t_train)
  nrmse_train_seed[i]=nrmse_train
  nrmse_test_seed[i]=nrmse_test
  
print("Train NRMSE with increasing sample size : ",nrmse_train_seed )
print("Test NRMSE with increasing sample size: ",nrmse_test_seed )
print("Train NRMSE Variance : ",np.var(nrmse_train_seed))
print("Train NRMSE Mean : ",np.mean(nrmse_train_seed))
print("Test NRMSE Variance : ",np.var(nrmse_test_seed))
print(" Test NRMSE Mean : ",np.mean(nrmse_test_seed))

# create the figure and axes
fig, ax = plt.subplots()
# create the boxplots
bp1 = ax.boxplot(nrmse_train_seed, positions=[1], widths=0.6)
bp2 = ax.boxplot(nrmse_test_seed, positions=[2], widths=0.6)
# customize the boxplots
for box in bp1['boxes']:
    box.set(color='blue', linewidth=2)
for box in bp2['boxes']:
    box.set(color='green', linewidth=2)
# set the x-axis labels
ax.set_xticklabels(['Train Data', 'Test Data'])
plt.show()
plt.title('Change in nrmse due to increase in sample size')
plt.xlabel("Samples")
plt.ylabel("NRMSE")
plt.plot(samples,nrmse_train_seed,label ='Train Data')
plt.plot(samples,nrmse_test_seed,label ='Test Data')
plt.legend(loc = "upper right")
plt.figure(figsize=(4, 2))
plt.show()

"""As number of samples are increased NRMSE first increases upto 100 samples than decrease linearly for both training and validation data.

Part b) Training and validation NRMSE obtained using pseudo inverse with number of variables.
In this part I have written two code
- First code represents change in NRMSE withc change in dimensions
- Second Code change in seed with dimension constant.
"""

#let n denote the no of samples(fixed=100) and d denote dimensions(variable)
d=10#Increase this value to see the sharp decrease
nrmse_train_seed=np.zeros(d)
nrmse_test_seed=np.zeros(d)

np.random.seed()
for i in range(1,d):
  np.random.seed()
  w_b = np.ones(i)
  in_x_b = input_x(1000,i)
  t_b=out_t(in_x_b,w_b)
  x_train, x_val, t_train, t_test = train_test_split(in_x_b, t_b, test_size=0.2)
  w_train,y_train,mse_pseudo_in_train=weight_pseudo_in(x_train,t_train,0.1)
  t_predict=lin_reg(x_val,w_train)
  nrmse_test=np.sqrt((mean_square_error(t_test,t_predict)))/np.var(t_test)
  nrmse_train=np.sqrt(mse_pseudo_in_train)/np.var(t_train)
  nrmse_train_seed[i]=nrmse_train
  nrmse_test_seed[i]=nrmse_test
  
print("Train NRMSE with increasing dimensions : ",nrmse_train_seed )
print("Test NRMSE with increasing dimensions: ",nrmse_test_seed )
print("Train NRMSE Variance : ",np.var(nrmse_train_seed))
print("Train NRMSE Mean : ",np.mean(nrmse_train_seed))
print("Test NRMSE Variance : ",np.var(nrmse_test_seed))
print(" Test NRMSE Mean : ",np.mean(nrmse_test_seed))

# create the figure and axes
fig, ax = plt.subplots()
# create the boxplots
bp1 = ax.boxplot(nrmse_train_seed, positions=[1], widths=0.6)
bp2 = ax.boxplot(nrmse_test_seed, positions=[2], widths=0.6)
# customize the boxplots
for box in bp1['boxes']:
    box.set(color='blue', linewidth=2)
for box in bp2['boxes']:
    box.set(color='green', linewidth=2)
# set the x-axis labels
ax.set_xticklabels(['Train Data', 'Test Data'])
plt.show()
plt.title('Change in nrmse due to increase in Dimensions')
plt.xlabel("Dimensions")
plt.ylabel("NRMSE")
plt.plot(np.arange(1,d),nrmse_train_seed[1:],label ='Train Data')
plt.plot(np.arange(1,d),nrmse_test_seed[1:],label ='Test Data')
plt.legend(loc = "upper right")
plt.figure(figsize=(4, 2))
plt.show()

"""Observation : 
- It can be easily seen that as number of dimension increases nrmse decreases very sharply. 
- Check values of NRMSE to verify.Don't see graph.
"""

#let n denote the no of samples(fixed=100) and d denote dimensions(variable)
d=30
n=10#number of different seeds
nrmse_train_seed=np.zeros(n)
nrmse_test_seed=np.zeros(n)
for i in range(1,n):
  np.random.seed(11+i)
  w_b = np.ones(d)
  in_x_b = input_x(1000,d)
  t_b=out_t(in_x_b,w_b)
  wz,y,mse_pseudo_in=weight_pseudo_in(in_x_b,t_b,1)
  x_train, x_val, t_train, t_test = train_test_split(in_x_b, t_b, test_size=0.2)
  w_train,y_train,mse_pseudo_in_train=weight_pseudo_in(x_train,t_train,0.1)
  t_predict=lin_reg(x_val,w_train)
  nrmse_test=np.sqrt((mean_square_error(t_test,t_predict)))/np.var(t_test)
  nrmse_train=np.sqrt(mse_pseudo_in_train)/np.var(t_train)
  nrmse_train_seed[i]=nrmse_train
  nrmse_test_seed[i]=nrmse_test

print("Train NRMSE with changing  seed : ",nrmse_train_seed )
print("Test NRMSE with changing seed : ",nrmse_test_seed )
print("Train NRMSE Variance : ",np.var(nrmse_train_seed))
print("Train NRMSE Mean : ",np.mean(nrmse_train_seed))
print("Test NRMSE Variance : ",np.var(nrmse_test_seed))
print(" Test NRMSE Mean : ",np.mean(nrmse_test_seed))

# create the figure and axes
fig, ax = plt.subplots()
# create the boxplots
bp1 = ax.boxplot(nrmse_train_seed, positions=[1], widths=0.6)
bp2 = ax.boxplot(nrmse_test_seed, positions=[2], widths=0.6)
# customize the boxplots
for box in bp1['boxes']:
    box.set(color='blue', linewidth=2)
for box in bp2['boxes']:
    box.set(color='green', linewidth=2)
# set the x-axis labels
ax.set_xticklabels(['Train Data', 'Test Data'])
plt.show()
plt.title('Change in nrmse due to seed')
plt.xlabel("Seed")
plt.ylabel("NRMSE")
plt.plot(np.arange(n),nrmse_train_seed,label ='Train Data')
plt.plot(np.arange(n),nrmse_test_seed,label ='Test Data')
plt.legend(loc = "upper right")
plt.figure(figsize=(4, 2))
plt.show()

"""Observation : 
- As seed increases for fixed number of dimensions nrmse changes very randomly and non linearly.
- Here we can also see that variance in test data is higher than variance in train data

c) Training and validation NRMSE obtained using pseudo inverse with noise variance
"""

#let n denote the noise variance(variable)
n=30

nrmse_train_seed=np.zeros(n)
nrmse_test_seed=np.zeros(n)
w_initialise=np.array([0.5,20,45,6,6])
in_x_c=input_x(50,5)#fixed samples  and dimension
w0=0
for i in range(n):
  t_c=out_t(in_x_c,w_initialise,w0,i)
  x_train, x_val, t_train, t_test = train_test_split(in_x_c, t_c, test_size=0.1)
  w_train,y_train,mse_pseudo_in_train=weight_pseudo_in(x_train,t_train,0.5)#lambda1=0.1
  t_predict=lin_reg(x_val,w_train)
  nrmse_test=np.sqrt((mean_square_error(t_test,t_predict)))/np.var(t_test)
  nrmse_train=np.sqrt(mse_pseudo_in_train)/np.var(t_train)
  nrmse_train_seed[i]=nrmse_train
  nrmse_test_seed[i]=nrmse_test
  
print("Train NRMSE with increasing Noise Variance : ",nrmse_train_seed )
print("Test NRMSE with increasing Noise Variance : ",nrmse_test_seed )
print("Train NRMSE Variance : ",np.var(nrmse_train_seed))
print("Train NRMSE Mean : ",np.mean(nrmse_train_seed))
print("Test NRMSE Variance : ",np.var(nrmse_test_seed))
print(" Test NRMSE Mean : ",np.mean(nrmse_test_seed))

# create the figure and axes
fig, ax = plt.subplots()
# create the boxplots
bp1 = ax.boxplot(nrmse_train_seed, positions=[1], widths=0.6)
bp2 = ax.boxplot(nrmse_test_seed, positions=[2], widths=0.6)
# customize the boxplots
for box in bp1['boxes']:
    box.set(color='blue', linewidth=2)
for box in bp2['boxes']:
    box.set(color='green', linewidth=2)
# set the x-axis labels
ax.set_xticklabels(['Train Data', 'Test Data'])
plt.show()
plt.title('Change in nrmse due to increase in Noise Variance')
plt.xlabel("Noise Variance")
plt.ylabel("NRMSE")
plt.plot(np.arange(n),nrmse_train_seed,label ='Train Data')
plt.plot(np.arange(n),nrmse_test_seed,label ='Test Data')
plt.legend(loc = "upper right")
plt.figure(figsize=(4, 2))
plt.show()

"""Observation : As noise variance increases we can see the sharp increase in NRMSE for both training and validation dataset.But the difference between two is that variance in test data is very high.
- By changing lambda one graph changes very much use value around 0.5 to see above effect
- By increasig lambda nrmse decreases but curve still follows the first point

d) Training and validation NRMSE obtained using pseudo inverse with w0
"""

#let n denote the no of samples(fixed=100) and d denote dimensions(fixed=5)
n=10
nrmse_train_seed=np.zeros(n)
nrmse_test_seed=np.zeros(n)
w_validate3=np.array([0.5,20,45,6,6])
in_x_d=input_x(100,5)
np.random.seed()
noise_var=0
for i in range(n):
  t_d=out_t(in_x_d,w_validate3,i,noise_var)
  x_train, x_val, t_train, t_test = train_test_split(in_x_d, t_d, test_size=0.2)
  w_train,y_train,mse_pseudo_in_train=weight_pseudo_in(x_train,t_train,0.8)
  t_predict=lin_reg(x_val,w_train)
  nrmse_test=np.sqrt((mean_square_error(t_test,t_predict)))/np.var(t_d)
  nrmse_train=np.sqrt(mse_pseudo_in_train)/np.var(t_d)
  nrmse_train_seed[i]=nrmse_train
  nrmse_test_seed[i]=nrmse_test
  
print("Train NRMSE with increasing Bias w0 : ",nrmse_train_seed )
print("Test NRMSE with increasing Bias w0 : ",nrmse_test_seed )
print("Train NRMSE Variance : ",np.var(nrmse_train_seed))
print("Train NRMSE Mean : ",np.mean(nrmse_train_seed))
print("Test NRMSE Variance : ",np.var(nrmse_test_seed))
print(" Test NRMSE Mean : ",np.mean(nrmse_test_seed))

# create the figure and axes
fig, ax = plt.subplots()
# create the boxplots
bp1 = ax.boxplot(nrmse_train_seed, positions=[1], widths=0.6)
bp2 = ax.boxplot(nrmse_test_seed, positions=[2], widths=0.6)
# customize the boxplots
for box in bp1['boxes']:
    box.set(color='blue', linewidth=2)
for box in bp2['boxes']:
    box.set(color='green', linewidth=2)
# set the x-axis labels
ax.set_xticklabels(['Train Data', 'Test Data'])
plt.show()
plt.title('Change in nrmse due to increase in Bias w0')
plt.xlabel("Bias w0")
plt.ylabel("NRMSE")
plt.plot(np.arange(n),nrmse_train_seed,label ='Train Data')
plt.plot(np.arange(n),nrmse_test_seed,label ='Test Data')
plt.legend(loc = "upper right")
plt.figure(figsize=(4, 2))
plt.show()

"""Observation :
- With change in noise variance graph changes very much #check for noise_var=0,3,10. For all of them NRMSE is increasing for increase in w0 
- Also keep lambda high,mostly greater than 0.5

e) Training and validation NRMSE obtained using pseudo inverse with lambda
"""

#let n denote the no of samples(fixed=100) and d denote dimensions(fixed=5)
#range of lambda 1 chosen from 0.000001 to 100
n=50
nrmse_train_seed=np.zeros(n)
nrmse_test_seed=np.zeros(n)
#lambda_one=np.array([-100,-10,-1,0.0001,.001,.01,.1,1,10,100])
lambda_one=np.linspace(0,1,n)
w_train = np.array([1,23,54,8,9])
in_x_e=input_x(500,5)
np.random.seed()
noise_var=1
w0=1
t_e=out_t(in_x_e,w_train,i,noise_var)
x_train, x_val, t_train, t_test = train_test_split(in_x_e, t_e, test_size=0.2)
for i in range(np.size(lambda_one)):
  w_train,y_train,mse_pseudo_in_train=weight_pseudo_in(x_train,t_train,lambda_one[i])
  t_predict=lin_reg(x_val,w_train)
  nrmse_test=np.sqrt((mean_square_error(t_test,t_predict)))/np.var(t_e)
  nrmse_train=np.sqrt(mse_pseudo_in_train)/np.var(t_e)
  nrmse_train_seed[i]=nrmse_train
  nrmse_test_seed[i]=nrmse_test
print("Train NRMSE with increasing lambda_one : ",nrmse_train_seed )
print("Test NRMSE with increasing lambda_one : ",nrmse_test_seed )
print("Train NRMSE Variance : ",np.var(nrmse_train_seed))
print("Train NRMSE Mean : ",np.mean(nrmse_train_seed))
print("Test NRMSE Variance : ",np.var(nrmse_test_seed))
print(" Test NRMSE Mean : ",np.mean(nrmse_test_seed))

# create the figure and axes
fig, ax = plt.subplots()
# create the boxplots
bp1 = ax.boxplot(nrmse_train_seed, positions=[1], widths=0.6)
bp2 = ax.boxplot(nrmse_test_seed, positions=[2], widths=0.6)
# customize the boxplots
for box in bp1['boxes']:
    box.set(color='blue', linewidth=2)
for box in bp2['boxes']:
    box.set(color='green', linewidth=2)
# set the x-axis labels
ax.set_xticklabels(['Train Data', 'Test Data'])
plt.show()
plt.title('Change in nrmse due to increase in lambda_one')
plt.xlabel("lambda_one")
plt.ylabel("NRMSE")
plt.plot(lambda_one,nrmse_train_seed,label ='Train Data')
plt.plot(lambda_one,nrmse_test_seed,label ='Test Data')
plt.legend(loc = "upper right")
plt.figure(figsize=(4, 2))
plt.show()

#it should look like u shaped

"""by changing lambda these curves changes very much try lesser lambda

f) Time taken to solve pseudo inverse with number of samples and number of variables and its breaking points
"""

import time
samples=np.array([10,100,1000,10000,100000,1000000])
time_count_train=np.zeros(np.size(samples))
time_count_test=np.zeros(np.size(samples))
np.random.seed()
w_train = np.array([1,23,54,8,9])
w_validate_f=np.array([0.5,20,45,6,6])
for i in range(np.size(samples)):
  in_x_f = input_x(samples[i],5)
  t_f=out_t(in_x_f,w_validate_f)
  x_train, x_val, t_train, t_test = train_test_split(in_x_f, t_f, test_size=0.2)
  tic = time.perf_counter()
  w_train,y_train,mse_pseudo_in_train=weight_pseudo_in(x_train,t_train,0.1)
  toc = time.perf_counter()
  time_count_train[i]=toc-tic
  tic = time.perf_counter()
  t_predict=lin_reg(x_val,w_train)
  toc = time.perf_counter()
  time_count_test[i]=toc-tic

plt.title('Time v/s Sample Size')
plt.xlabel("Samples")
plt.ylabel("Time")
plt.plot(samples,time_count_train,label ='Train Data')
plt.plot(samples,time_count_test,label ='Test Data')
plt.legend(loc = "upper right")
plt.figure(figsize=(4, 2))
plt.show()

"""With increasing samples upto $10^7$ terms program is running but for $10^9$ samples it lasted more than 30 minutes until interupted.It is nonlinear upto 1000 samples and after that it is completely linear.

g) Training and validation NRMSE obtained using gradient descent with max_iter
"""

n= 200 #Here n is max_iter(variable)

nrmse_train_seed=np.zeros(n)
nrmse_test_seed=np.zeros(n)
np.random.seed(1)
in_x_g = input_x(1000,8)
w_g = np.array([11,23,54,8,9,33,46,78])
t=out_t(in_x_g,w_g)
eta = 0.0001
lambda2 = 0
lambda1 = 1
min_change_NRMSE = 0.0000001
x_train, x_val, t_train, t_test = train_test_split(in_x_g, t, test_size=0.2)
for i in range(1,n):
  final_w_train, final_nrmse_train ,no_iter_train= linear_regression_grad_desc(x_train, t_train, w_g, eta, lambda2, lambda1, min_change_NRMSE,i)
  #print(no_iter_train)
  t_predict=lin_reg(x_val,final_w_train)
  final_nrmse_test=np.sqrt((mean_square_error(t_test,t_predict)))/np.var(t_test)
  nrmse_train_seed[i]=final_nrmse_train
  nrmse_test_seed[i]=final_nrmse_test

print("Train NRMSE with increasing max_iter : ",nrmse_train_seed )
print("Test NRMSE with increasing max_iter : ",nrmse_test_seed )
print("Train NRMSE Variance : ",np.var(nrmse_train_seed))
print("Train NRMSE Mean : ",np.mean(nrmse_train_seed))
print("Test NRMSE Variance : ",np.var(nrmse_test_seed))
print(" Test NRMSE Mean : ",np.mean(nrmse_test_seed))

# create the figure and axes
fig, ax = plt.subplots()
# create the boxplots
bp1 = ax.boxplot(nrmse_train_seed, positions=[1], widths=0.6)
bp2 = ax.boxplot(nrmse_test_seed, positions=[2], widths=0.6)
# customize the boxplots
for box in bp1['boxes']:
    box.set(color='blue', linewidth=2)
for box in bp2['boxes']:
    box.set(color='green', linewidth=2)
# set the x-axis labels
ax.set_xticklabels(['Train Data', 'Test Data'])
plt.show()
plt.title('Change in nrmse due to increase in max_iter')
plt.xlabel("max_iter")
plt.ylabel("NRMSE")
plt.plot(np.arange(n),nrmse_train_seed,label ='Train Data')
plt.plot(np.arange(n),nrmse_test_seed,label ='Test Data')
plt.legend(loc = "upper right")
plt.figure(figsize=(4, 2))
plt.show()
#it should monotonically decrease

"""Observation : we can say that max_iter doesn't have difference between test and train data at lower values upto 200.

h) Training and validation NRMSE obtained using gradient descent with eta
"""

n= 11 #Here n is number of  eta(variable)
eta=np.array([0.00000000001,0.000000001,0.00000001,0.00000001,.000001,0.00001,.0001,0.001,0.01,.1,1])#Size of this matrix should be eqal to #n
nrmse_train_seed=np.zeros(n)
nrmse_test_seed=np.zeros(n)
np.random.seed(1)
in_x_h = input_x(200,5)
w_h = np.array([1,3,4,8,9])
w0=2
t=out_t(in_x_h,w_h,w0,10)
x_train, x_val, t_train, t_test = train_test_split(in_x_h, t, test_size=0.2)
lambda2 = 1
lambda1 = 1
min_change_NRMSE = 0.000000001
max_iter=10000
for i in range(n):
  final_w_train, final_nrmse_train ,no_iter_train= linear_regression_grad_desc(x_train, t_train, w_h, eta[i], lambda2, lambda1, min_change_NRMSE,max_iter)
  t_predict=lin_reg(x_val,final_w_train)
  final_nrmse_test=np.sqrt((mean_square_error(t_test,t_predict)))/np.var(t_test)
  nrmse_train_seed[i]=final_nrmse_train
  nrmse_test_seed[i]=final_nrmse_test
print("Train NRMSE with increasing eta : ",nrmse_train_seed )
print("Test NRMSE with increasing eta : ",nrmse_test_seed )
print("Train NRMSE Variance : ",np.var(nrmse_train_seed))
print("Train NRMSE Mean : ",np.mean(nrmse_train_seed))
print("Test NRMSE Variance : ",np.var(nrmse_test_seed))
print(" Test NRMSE Mean : ",np.mean(nrmse_test_seed))

# create the figure and axes
fig, ax = plt.subplots()
# create the boxplots
bp1 = ax.boxplot(nrmse_train_seed, positions=[1], widths=0.6)
bp2 = ax.boxplot(nrmse_test_seed, positions=[2], widths=0.6)
# customize the boxplots
for box in bp1['boxes']:
    box.set(color='blue', linewidth=2)
for box in bp2['boxes']:
    box.set(color='green', linewidth=2)
# set the x-axis labels
ax.set_xticklabels(['Train Data', 'Test Data'])
plt.show()
plt.title('Change in nrmse due to increase in eta')
plt.xlabel("eta")
plt.ylabel("(NRMSE)")
plt.plot(eta,(nrmse_train_seed),label ='Train Data')
plt.plot(eta,(nrmse_test_seed),label ='Test Data')
plt.legend(loc = "upper right")
plt.figure(figsize=(4, 2))
plt.show()

"""Observation : 
- As eta increases nrmse initially decreases and then increases abrubtly very high rate above eta 0.01 nrmse becomes very high (infinity)
- nrmse in test is higher than with the train data but the difference is almost constant

i) Time taken to solve gradient descent with number of samples and number of variables and its breaking points
"""

n= 10000 #Here n is number of samples(variable)
samples=np.array([10,100,1000,10000,100000,1000000])
time_count_train=np.zeros(np.size(samples))
time_count_test=np.zeros(np.size(samples))
nrmse=np.zeros(n)
np.random.seed(1)
w_i = np.array([11,23,54,8,9,33,46,78])
eta = 0.000001
lambda2 = 0.1
lambda1 = 0.1
min_change_NRMSE = 0.000000001
max_iter=500
for i in range(np.size(samples)):
  in_x_i = input_x(samples[i],8)
  t_i=out_t(in_x_i,w_i)
  x_train, x_val, t_train, t_test = train_test_split(in_x_i, t_i, test_size=0.2)
  final_w, final_nrmse ,no_iter= linear_regression_grad_desc(in_x_i, t_i, w_i, eta, lambda2, lambda1, min_change_NRMSE,max_iter)
  tic = time.perf_counter()
  w_train,y_train,mse_pseudo_in_train=weight_pseudo_in(x_train,t_train,0.1)
  toc = time.perf_counter()
  time_count_train[i]=toc-tic
  tic = time.perf_counter()
  w_test,y_test,mse_pseudo_in_test=weight_pseudo_in(x_val,t_test,0.1)
  toc = time.perf_counter()
  time_count_test[i]=toc-tic

plt.title('Time v/s Sample Size')
plt.xlabel("Samples")
plt.ylabel("Time")
plt.plot(samples,time_count_train,label ='Train Data')
plt.plot(samples,time_count_test,label ='Test Data')
plt.legend(loc = "upper right")
plt.figure(figsize=(4, 2))
plt.show()

"""Observation : As number of samples increases time taken increases linearly.

j) Time taken to solve gradient descent with number of variables and its breaking point \\
**Skipped**

k) Training and validation NRMSE and number of nearly zero weights obtained using gradient descent with lambda2
"""

n= 10 #Here n is number of lambda 2 values(variable)
lambda2=np.linspace(0,1,n)#Size of this matrix should be eqal to #n
nrmse_train_seed=np.zeros(n)
nrmse_test_seed=np.zeros(n)
np.random.seed(1)
in_x_k = input_x(500,5)
w_k = np.array([.01,.03,.046,.088,.19])#nearly zero weights
t_k=out_t(in_x_k,w_k)
x_train, x_val, t_train, t_test = train_test_split(in_x_k, t_k, test_size=0.2)
eta = 0.00001
lambda1 = 0.1
min_change_NRMSE = 0.0000001
max_iter=100000
for i in range(n):
  final_w_train, final_nrmse_train ,no_iter_train= linear_regression_grad_desc(x_train, t_train, w_k, eta, lambda2[i], lambda1, min_change_NRMSE,max_iter)
  t_predict=lin_reg(x_val,final_w_train)
  final_nrmse_test=np.sqrt((mean_square_error(t_test,t_predict)))/np.var(t_test)
  nrmse_train_seed[i]=final_nrmse_train
  nrmse_test_seed[i]=final_nrmse_test

print("Train NRMSE with increasing lambda2 : ",nrmse_train_seed )
print("Test NRMSE with increasing lambda2 : ",nrmse_test_seed )
print("Train NRMSE Variance : ",np.var(nrmse_train_seed))
print("Train NRMSE Mean : ",np.mean(nrmse_train_seed))
print("Test NRMSE Variance : ",np.var(nrmse_test_seed))
print(" Test NRMSE Mean : ",np.mean(nrmse_test_seed))

# create the figure and axes
fig, ax = plt.subplots()
# create the boxplots
bp1 = ax.boxplot(nrmse_train_seed, positions=[1], widths=0.6)
bp2 = ax.boxplot(nrmse_test_seed, positions=[2], widths=0.6)
# customize the boxplots
for box in bp1['boxes']:
    box.set(color='blue', linewidth=2)
for box in bp2['boxes']:
    box.set(color='green', linewidth=2)
# set the x-axis labels
ax.set_xticklabels(['Train Data', 'Test Data'])
plt.show()
plt.title('Change in nrmse due to increase in lambda2')
plt.xlabel("lambda2")
plt.ylabel("NRMSE")
plt.plot(lambda2,nrmse_train_seed,label ='Train Data')
plt.plot(lambda2,nrmse_test_seed,label ='Test Data')
plt.legend(loc = "upper right")
plt.figure(figsize=(4, 2))
plt.show()

"""Observation :
- Optimal lambda2 is coming out to be 1.
- Error increases non linearly(looks like quadratic) with increase in

l) Training and validation NRMSE and number of nearly zero weights obtained using gradient descent with lambda1
"""

n= 20 #Here n is number of lambda 1 values(variable)
#lambda1=np.array([0.001,0.01,0.1,1,5,10,100,150,200,300,318,330,350,400,500,1000,2000,3000])#Size of this matrix should be eqal to #n
lambda1=np.linspace(0,1,n)#Size of this matrix should be eqal to #n
nrmse_train_seed=np.zeros(n)
nrmse_test_seed=np.zeros(n)
np.random.seed(1)
in_x_l = input_x(5000,5)#dont change input
w_l = np.array([.01,.2,.14,.03,.09])
t_l=out_t(in_x_l,w_l)
x_train, x_val, t_train, t_test = train_test_split(in_x_l, t_l, test_size=0.2)
eta = 0.0001
lambda2 = 1
min_change_NRMSE = 0.0000001
max_iter=1000
for i in range(n):
  final_w_train, final_nrmse_train ,no_iter_train= linear_regression_grad_desc(x_train, t_train, w_l, eta, lambda2, lambda1[i], min_change_NRMSE,max_iter)
  t_predict=lin_reg(x_val,final_w_train)
  final_nrmse_test=np.sqrt((mean_square_error(t_test,t_predict)))/np.var(t_test)
  nrmse_train_seed[i]=final_nrmse_train
  nrmse_test_seed[i]=final_nrmse_test

print("Train NRMSE with increasing lambda1 : ",nrmse_train_seed )
print("Test NRMSE with increasing lambda1 : ",nrmse_test_seed )
print("Train NRMSE Variance : ",np.var(nrmse_train_seed))
print("Train NRMSE Mean : ",np.mean(nrmse_train_seed))
print("Test NRMSE Variance : ",np.var(nrmse_test_seed))
print(" Test NRMSE Mean : ",np.mean(nrmse_test_seed))

# create the figure and axes
fig, ax = plt.subplots()
# create the boxplots
bp1 = ax.boxplot(nrmse_train_seed, positions=[1], widths=0.6)
bp2 = ax.boxplot(nrmse_test_seed, positions=[2], widths=0.6)
# customize the boxplots
for box in bp1['boxes']:
    box.set(color='blue', linewidth=2)
for box in bp2['boxes']:
    box.set(color='green', linewidth=2)
# set the x-axis labels
ax.set_xticklabels(['Train Data', 'Test Data'])
plt.show()
plt.title('Change in nrmse due to increase in lambda1')
plt.xlabel("lambda1")
plt.ylabel("NRMSE")
plt.plot(lambda1,nrmse_train_seed,label ='Train Data')
plt.plot(lambda1,nrmse_test_seed,label ='Test Data')
plt.legend(loc = "upper right")
plt.figure(figsize=(4, 2))
plt.show()

"""Observation : 
- Optimum value of lambda1 turns to be around 0 \
- Change in eta causes lot of change in characteristic curve(dont decrease too much)

m) Training and validation NRMSE for optimal lambda2 with noise variance
"""

n= 20 #Here n is number of noise variance values(variable)
#Size of this matrix should be eqal to #n
nrmse_train_seed=np.zeros(n)
nrmse_test_seed=np.zeros(n)
np.random.seed(1)
in_x_l = input_x(1000,5)#dont change input
w_l = np.array([1,2,4,3,.29])
w0=10
eta = 0.000001
lambda2 = 1 #optimal
lambda1 = 0.1
min_change_NRMSE = 0.00000001
max_iter=1000
for i in range(n):
  t_l=out_t(in_x_l,w_l,w0,i)
  x_train, x_val, t_train, t_test = train_test_split(in_x_l, t_l, test_size=0.2)
  final_w_train, final_nrmse_train ,no_iter_train= linear_regression_grad_desc(x_train, t_train, w_l, eta, lambda2, lambda1, min_change_NRMSE,max_iter)
  t_predict=lin_reg(x_val,final_w_train)
  final_nrmse_test=np.sqrt((mean_square_error(t_test,t_predict)))/np.var(t_test)
  nrmse_train_seed[i]=final_nrmse_train
  nrmse_test_seed[i]=final_nrmse_test

print("Train NRMSE with increasing noise Variance : ",nrmse_train_seed )
print("Test NRMSE with increasing noise Variance : ",nrmse_test_seed )
print("Train NRMSE Variance : ",np.var(nrmse_train_seed))
print("Train NRMSE Mean : ",np.mean(nrmse_train_seed))
print("Test NRMSE Variance : ",np.var(nrmse_test_seed))
print(" Test NRMSE Mean : ",np.mean(nrmse_test_seed))

# create the figure and axes
fig, ax = plt.subplots()
# create the boxplots
bp1 = ax.boxplot(nrmse_train_seed, positions=[1], widths=0.6)
bp2 = ax.boxplot(nrmse_test_seed, positions=[2], widths=0.6)
# customize the boxplots
for box in bp1['boxes']:
    box.set(color='blue', linewidth=2)
for box in bp2['boxes']:
    box.set(color='green', linewidth=2)
# set the x-axis labels
ax.set_xticklabels(['Train Data', 'Test Data'])
plt.show()
plt.title('Change in nrmse due to increase in Noise Variance')
plt.xlabel("Noise Variance")
plt.ylabel("NRMSE")
plt.plot(np.arange(n),nrmse_train_seed,label ='Train Data')
plt.plot(np.arange(n),nrmse_test_seed,label ='Test Data')
plt.legend(loc = "upper right")
plt.figure(figsize=(4, 2))
plt.show()

"""Observation : As noise variance increases both test and train NRMSE increases and the variance in test data is much higher than that of train data.

n) Training and validation NRMSE for optimal lambda1 with noise variance
"""

n= 50 #Here n is number of noise variance values(variable)
#Size of this matrix should be eqal to #n
nrmse_train_seed=np.zeros(n)
nrmse_test_seed=np.zeros(n)
np.random.seed(1)
in_x_l = input_x(1000,5)#dont change input
w_l = np.array([1,.2,.4,3,.29])
w0=10
eta = 0.001
lambda2 = 1 #optimal
lambda1 = 0.1#taken near to zero
min_change_NRMSE = 0.00000001
max_iter=1000
for i in range(n):
  t_l=out_t(in_x_l,w_l,w0,i)
  x_train, x_val, t_train, t_test = train_test_split(in_x_l, t_l, test_size=0.2)
  final_w_train, final_nrmse_train ,no_iter_train= linear_regression_grad_desc(x_train, t_train, w_l, eta, lambda2, lambda1, min_change_NRMSE,max_iter)
  t_predict=lin_reg(x_val,final_w_train)
  final_nrmse_test=np.sqrt((mean_square_error(t_test,t_predict)))/np.var(t_test)
  nrmse_train_seed[i]=final_nrmse_train
  nrmse_test_seed[i]=final_nrmse_test

print("Train NRMSE with increasing noise Variance : ",nrmse_train_seed )
print("Test NRMSE with increasing noise Variance : ",nrmse_test_seed )
print("Train NRMSE Variance : ",np.var(nrmse_train_seed))
print("Train NRMSE Mean : ",np.mean(nrmse_train_seed))
print("Test NRMSE Variance : ",np.var(nrmse_test_seed))
print(" Test NRMSE Mean : ",np.mean(nrmse_test_seed))

# create the figure and axes
fig, ax = plt.subplots()
# create the boxplots
bp1 = ax.boxplot(nrmse_train_seed, positions=[1], widths=0.6)
bp2 = ax.boxplot(nrmse_test_seed, positions=[2], widths=0.6)
# customize the boxplots
for box in bp1['boxes']:
    box.set(color='blue', linewidth=2)
for box in bp2['boxes']:
    box.set(color='green', linewidth=2)
# set the x-axis labels
ax.set_xticklabels(['Train Data', 'Test Data'])
plt.show()
plt.title('Change in nrmse due to increase in Noise Variance')
plt.xlabel("Noise Variance")
plt.ylabel("NRMSE")
plt.plot(np.arange(n),nrmse_train_seed,label ='Train Data')
plt.plot(np.arange(n),nrmse_test_seed,label ='Test Data')
plt.legend(loc = "upper right")
plt.figure(figsize=(4, 2))
plt.show()

"""Observation : 
- Firstly with noise variance both test and train data NRMSE increases upto variance 10 and then decreases fastly.
- Here also variance in test data is higher than that of train data.

o) Experiment (f) but, this time with number of training samples and number of variables \
**Skipped**

#### **Learning Points**
1. First and most important learning is that don't use the randomly generated data for analysis because it will create more confusion whether your code is correct or not.
2. Time taken for same samples and dimension for linear regression using pseudo inverse method is way less than time taken using gradient descent method.This can be understood by code size also.
3. Learned how to split the input data into training and validation sets and how to validate the model using the validation set.
4. Learned how to plot Boxplot for various distributions.
5. Learned Linear Regression from Scratch using two methods Pseudo Inverse with L2 Regularization and Gradiet Descent method.
6. Learned how to manage and vary the hyperparameters to get the optimal results.

Resources :
1. https://towardsdatascience.com/simple-linear-regression-in-python-numpy-only-130a988c0212
2. https://towardsdatascience.com/gradient-descent-from-scratch-e8b75fa986cc
3. https://youtu.be/xrPZbHrxrWo
4. https://towardsdatascience.com/linear-regression-using-gradient-descent-97a6c8700931
5. Boxplot code taken from : https://matplotlib.org/stable/gallery/statistics/boxplot_demo.html
6. Understood code snippets from above sources on ChatGPT

Code Discussed with friends : Only discussed 13th question theory and maths implementation part.
1. Dinesh Kumar Panwar 22M1080
2. Karra Maneesha 22M1076
"""